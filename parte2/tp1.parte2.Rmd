---
title: Data Mining en Ciencia y Técnica - TP1
author: Ariel Aguirre, Miguel Barros, José Badillo, Diego Dell'Era
output: pdf_document
---

TP1 - parte 2
=============

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(knitr)
library(markdown)
# para armar el documento:
# knit('tp1.parte2.Rmd', 'tp1.parte2.md'); markdownToHTML('tp1.parte2.md', 'tp1.parte2.html')
```

## Tarea 1

Aplicamos los pasos de la parte 1.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# leer dataset y corregir formato de variable no numérica
glx <- read.csv("COMBO17.csv", header = T, stringsAsFactors = F)
glx$e.W420FE <- as.numeric(glx$e.W420FE)

# quitar outliers
glx_sin_outliers <- subset(glx, ApDRmag > -3.2)
glx_sin_outliers <- subset(glx_sin_outliers, BjMAG < -7.0)
glx_sin_outliers <- subset(glx_sin_outliers, BbMAG < -9.0)
glx_sin_outliers <- subset(glx_sin_outliers, UjMAG < -10.0)
glx_sin_outliers <- subset(glx_sin_outliers, UbMAG < -10.0)
glx_sin_outliers <- subset(glx_sin_outliers, VjMAG < -10.0)
glx_sin_outliers <- subset(glx_sin_outliers, VnMAG < -10.0)
glx_sin_outliers <- subset(glx_sin_outliers, usMAG < -10.0)
glx_sin_outliers <- subset(glx_sin_outliers, gsMAG < -9.0)
glx_sin_outliers <- subset(glx_sin_outliers, rsMAG < -9.0)

# quitar faltantes
variables_de_interes <- c(1,2,3,4,10:29)
glx_sin_faltantes <- glx_sin_outliers[complete.cases(glx_sin_outliers[,variables_de_interes]),]

# guardar dataset procesado (sólo columnas con variables de interés)
variables_de_interes_finales <- c(1,2,4,6,10,12,14,16,18,20,22,24,26,28)
glx_procesado_parte_1 <- glx_sin_faltantes[,variables_de_interes_finales]
names(glx_procesado_parte_1) <- c("nr", "rmag", "apdrmag", "mcz", "ujmag", "bjmag", "vjmag", "usmag", "gsmag", "rsmag", "ubmag", "bbmag", "vbmag", "s280mag")
write.csv(glx_procesado_parte_1, "dataset.procesado.parte.1.csv", row.names = FALSE)
```

```{r, results='markup', warning=FALSE, message=FALSE}
glx_0 <- read.csv("dataset.procesado.parte.1.csv", header = T, stringsAsFactors = F)
head(glx_0, 1)
```

Analizamos agrupamientos usando k-medias. Pasos:

* Quitamos variables correlacionadas

Las variables que tienen índice de correlación 1, según los resultados de la parte 1, son:

```{r, results='markup', warning=FALSE, message=FALSE}
# (ujmag, usmag, ubmag)
# (bjmag, bbmag)
# (vjmag, rsmag, vnmag)
```

Se trata, obviamente, de los grupos de magnitudes medidas en una misma banda. Nos quedamos con 1 variable que represente a cada grupo:

```{r, results='markup', warning=FALSE, message=FALSE}
glx_1 <- glx_0[,c(1:7)]
```

* Estandarizamos variables

```{r, results='markup', warning=FALSE, message=FALSE}
# sólo estandarizamos variables con mediciones (sin la primera columna, porque es el número de galaxia)
glx_2 <- scale(glx_1[,c(2:7)])
head(glx_2, 1)
```

* Determinamos el `k` óptimo para K-means:

- Gráficamente

Podemos ver el `k` óptimo como el punto de corte en que deja de disminuir marcadamente la suma de cuadrados dentro del cluster. En otras palabras, el punto en que la cohesión de los clusters deja de aumentar.

```{r, results='markup', warning=FALSE, message=FALSE}
wssplot <- function(data, nc=15, seed=1234){wss <- (nrow(data)-1)*sum(apply(data,2,var))
               for (i in 2:nc){
                    set.seed(seed)
                    wss[i] <- sum(kmeans(data, centers=i)$withinss)}
                plot(1:nc, wss, type="b", xlab="Número de clusters",
                     ylab="Suma de cuadrados dentro del cluster")}

wssplot(glx_2)
```

El gráfico sugiere usar `k` = 2 ó 3 (es discutible).

- Analíticamente

Calculamos una matrix de distancias:

```{r, results='markup', warning=FALSE, message=FALSE}
library(cluster)
glx_2_dist <- dist(glx_2)
```

Usamos la matriz de distancias con `kmeans`:

```{r, results='markup', warning=FALSE, message=FALSE}
# una función que, dado un k, clusteriza por kmeans -> calcula silhouette -> devuelve ancho promedio
silhoutte_kmeans <- function(k) {
  c <- kmeans(glx_2, centers = k)$cluster
  s <- silhouette(c, glx_2_dist)
  return(summary(s)$avg.width)
}

# aplicamos la función para valores de k entre 2 y 10
indice_por_k_kmeans <- sapply(c(2:10), silhoutte_kmeans)
which.max(indice_por_k_kmeans)
```

El `k` que maximiza el ancho promedio de silhouette para todos los clusters creados via K-means es el primero => `k` = 2.

Hay 2 grupos. Leyendo un poco la descripción del dataset, una interpretación posible es que los grupos representen galaxias 'azules' y 'rojas'; en otras palabras, que los grupos dependen del corrimiento al rojo. Si lo ploteamos, las medias difieren bastante:

```{r, results='markup', warning=FALSE, message=FALSE}
k2 <- kmeans(glx_2, centers = 2)
boxplot(glx_1$mcz ~ k2$cluster, col=c("red", "blue"))
```

Éstos son los centros de cada cluster (revirtiendo a las medidas originales). Se ve que el redshift mayor o menor que 0 divide a los grupos:

```{r, results='markup', warning=FALSE, message=FALSE}
aggregate(glx_2, by = list(k2$cluster), mean)
```

## Tarea 1 (optativa): Comparar métodos

Ahora usamos PAM en lugar de K-means:

```{r, results='markup', warning=FALSE, message=FALSE}
# una función que, dado un k, clusteriza por PAM -> calcula silhouette -> devuelve ancho promedio
silhoutte_pam <- function(k) {
  c <- pam(glx_2_dist, k=k, diss=T)
  s <- silhouette(c)
  return(summary(s)$avg.width)
}

# aplicamos la función para valores de k entre 2 y 5
indice_por_k_pam <- sapply(c(2:5), silhoutte_pam)
which.max(indice_por_k_pam)
```

El `k` que maximiza el ancho promedio de silhouette para todos los clusters creados via PAM es el primero => `k` = 2.

Ventajas de PAM sobre K-means:

* es más robusto ante la presencia de outliers (porque la mediana es más robusta que la media);
* se puede usar cualquier medida de similitud;
* como usa medoides, encuentra objetos representativos del cluster => el resultado se puede interpretar mejor estudiando las propiedades de un objeto particular.

Principal desventaja de PAM ante K-means:

* es más lento, porque tiene una complejidad del orden de O(n^2 * k * i) (donde n = cantidad de objetos, k = cantidad de medoides, i = iteraciones). K-means, en cambio, tiene una complejidad de O(n * k * i) (evita el término cuadrático porque sólo tiene que calcular distancias de los objetos a los centroides).

También podemos probar otra biblioteca para clusterizar maximizando el índice de Silhouette:

```{r, results='markup', warning=FALSE, message=FALSE}
library(NbClust)
set.seed(1234)
nc <- NbClust(glx_2, min.nc = 2, max.nc = 5, method = "kmeans", index = "silhouette")
nc$Best.nc
```

Este método también sugiere usar `k` = 2.

## Tarea 2

Cargamos el dataset entero (partimos de un .csv que fue creado a partir del dataset original descargado del website y parseado según las instrucciones para cada campo):

```{r, results='markup', warning=FALSE, message=FALSE}
glx <- read.csv("dmcyt_tp2.csv", header = T, stringsAsFactors = F, sep = '|')
glx$mc_class <- as.factor(glx$mc_class)

# evitemos que R trunque decimales
options(digits=16)
```

Quitamos outliers:

```{r, results='markup', warning=FALSE, message=FALSE}
glx_sin_outliers <- subset(glx, apd_rmag > -3.2)
glx_sin_outliers <- subset(glx_sin_outliers, bjmag < -7.0)
glx_sin_outliers <- subset(glx_sin_outliers, ujmag < -10.0)
glx_sin_outliers <- subset(glx_sin_outliers, vjmag < -10.0)
glx_sin_outliers <- subset(glx_sin_outliers, usmag < -10.0)
glx_sin_outliers <- subset(glx_sin_outliers, gsmag < -9.0)
glx_sin_outliers <- subset(glx_sin_outliers, rsmag < -9.0)
# glx_sin_outliers <- subset(glx_sin_outliers, bbmag < -9.0) # todos los valores en 0?
# glx_sin_outliers <- subset(glx_sin_outliers, ubmag < -10.0) # todos los valores en 0?
# glx_sin_outliers <- subset(glx_sin_outliers, vbmag < -10.0) # todos los valores en 0? (antes era vnmag)

nrow(glx) - nrow(glx_sin_outliers)
```

Buscamos datos faltantes:

```{r, results='markup', warning=FALSE, message=FALSE}
glx_sin_faltantes <- glx_sin_outliers[complete.cases(glx_sin_outliers),]
nrow(glx_sin_outliers) - nrow(glx_sin_faltantes)
```

Dimensiones del dataset final:

```{r, results='markup', warning=FALSE, message=FALSE}
dim(glx_sin_faltantes)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# Guardamos el dataset. Sólo van con comillas la clase ('mc_class') y el signo de la declinación ('de_'); el resto son números
# write.table(format(glx_sin_faltantes, digits=16), file = "dmcyt_tp2.tarea_2.csv", row.names = FALSE, na="", col.names = TRUE, sep="|", quote = c(5, 15))
```

## Tarea 3

```{r, echo=FALSE, warning=FALSE, message=FALSE}
glx_sin_faltantes <- read.csv("dmcyt_tp2.tarea_2.csv", sep="|")
```

Extraemos las variables que tienen mediciones para poder encontrar clusters:

```{r, results='markup', warning=FALSE, message=FALSE}
glx_tarea3.0 <- glx_sin_faltantes[,c(11,13,17,18,19)]
```

Ahora calculamos distancias para correr PAM y después Silhouette:

```{r, results='markup', warning=FALSE, message=FALSE}
# glx_tarea3.1 <- dist(scale(glx_tarea3.0))
```

Pinchó. ¿Por qué?

Calcular la matriz de distancias implica que, por cada elemento de la matriz, hay que calcular la distancia a todos los demás elementos. Es decir, es un procedimiento cuadrático: si la matriz tiene `n` elementos, hay que almacenar `n x n` distancias.

Se puede deducir, para el caso de la implementación de R, dónde está el problema de alocación de memoria:

```{r, results='markup', warning=FALSE, message=FALSE}
# tomamos las primeras 16000 filas
glx_muestra_16k <- glx_sin_faltantes[c(1:16000), c(11,13,17,18,19)]

# intentamos calcular la matriz, y la imprimimos para forzarlo a que nos devuelve un mensaje de error
# dist(scale(glx_muestra_16k))
```

Por el mensaje de error, vemos que no puede alocar espacio (0.975 Gb). Entonces podemos despejar la cantidad de bytes que usa para guardar distancias:

```{r, results='markup', warning=FALSE, message=FALSE}
0.975 * (1024)^3 / (16000^2)
```

Son 4 bytes. Usando apenas 4 bytes para almacenar distancias, el cálculo para una matriz de tamaño modesto se vuelve demasiado costoso, rápidamente.

Podemos probar de nuevo restringiendo el tamaño del dataset (y limpiando un poco con gc()).
```{r, echo=FALSE, warning=FALSE, message=FALSE}
gc()
```

Tomamos una muestra de 10000 objetos (en la compu donde corre esto, si tomamos más de 15000 pincha). Agrupamos con K-means buscando el `k` óptimo:

```{r, results='markup', warning=FALSE, message=FALSE}
filas_muestra <- sample(nrow(glx_sin_faltantes), size = 10000)
glx_muestra_10k <- glx_sin_faltantes[filas_muestra,]
glx_muestra_10k_mediciones <- glx_muestra_10k[,c(11,13,16,17,18,19)]
head(glx_muestra_10k_mediciones, 1)
```

```{r, results='markup', warning=FALSE, message=FALSE}
glx_muestra_10k_dist <- dist(scale(glx_muestra_10k_mediciones))

silhoutte_kmeans <- function(k) {
  c <- kmeans(glx_muestra_10k_mediciones, centers = k)$cluster
  s <- silhouette(c, glx_muestra_10k_dist)
  return(summary(s)$avg.width)
}

# aplicamos la función para valores de k entre 2 y 5
indice_por_k_kmeans <- sapply(c(2:5), silhoutte_kmeans)
which.max(indice_por_k_kmeans)
```

Obtenemos el primer k, o sea `k` = 2.

(Sería interesante investigar si el "valle verde" que está entre las galaxias rojas y la nube de galaxias azules en el gráfico de color-magnitud aparecería ampliando la muestra...)
